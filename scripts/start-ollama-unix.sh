#!/bin/bashecho "Starting Ollama setup..."# Set environment variables to fix model path issuesexport OLLAMA_MODELS="/root/.ollama/models"export OLLAMA_ORIGINS="*"# Read model configuration from config.ymlif [ -f /config.yml ]; then    echo "Reading model configuration from config.yml..."    CONFIGURED_MODELS=$(python3 -c 'import yamlimport systry:    with open("/config.yml", "r") as f:        config = yaml.safe_load(f)    if config and "llm" in config and "ollama_models" in config["llm"]:        if isinstance(config["llm"]["ollama_models"], list):            for model in config["llm"]["ollama_models"]:                print(model)        elif isinstance(config["llm"]["ollama_models"], str):            print(config["llm"]["ollama_models"])except Exception as e:    print(f"Error: {e}", file=sys.stderr)    sys.exit(1)')    echo "The following models should be available from the build process:"    echo "$CONFIGURED_MODELS" | while read -r model; do        echo "- $model"    doneelse    echo "No config.yml found. Using default models."    CONFIGURED_MODELS="deepseek-r1:1.5bdeepseek-r1:7b"    echo "- deepseek-r1:1.5b"    echo "- deepseek-r1:7b"fi# Start Ollama in the background to pull modelsecho "Starting Ollama server in the background..."ollama serve > /dev/null 2>&1 &SERVER_PID=$!# Wait for Ollama to startecho "Waiting for Ollama server to start..."sleep 5# Check if models exist and pull them if they don'techo "Checking for missing models and pulling them if necessary..."echo "$CONFIGURED_MODELS" | while read -r model; do    if ollama list | grep -q "$model"; then        echo "Model $model is already available."    else        echo "Model $model is missing. Pulling now..."        ollama pull "$model"        echo "Finished pulling $model."    fidone# Kill background Ollama serverkill $SERVER_PIDsleep 2echo "Actual available models:"ollama list# Start Ollama serviceecho "Starting Ollama server..."exec ollama serve